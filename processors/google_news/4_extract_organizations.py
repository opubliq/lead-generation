#!/usr/bin/env python3
"""
√âtape 4: Extrait et agr√®ge les organisations mentionn√©es dans les articles
Input:  data/warehouse/google_news_<date>.csv
Output: data/warehouse/google_news_organizations_<date>.json
        data/warehouse/google_news_summaries_<date>.json
"""

import csv
import json
from pathlib import Path
from datetime import datetime
import google.generativeai as genai
import os
from time import sleep
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed


# Configuration Gemini
GEMINI_MODEL = "gemini-2.5-flash"
MAX_WORKERS = 4  # Nombre de threads parall√®les pour Gemini

# Prompt pour extraction d'organisations ET r√©sum√©
EXTRACTION_PROMPT = """Analyse cet article de presse qu√©b√©cois et extrais les informations suivantes.

TITRE DE L'ARTICLE: {titre}

CONTENU: {contenu}

INSTRUCTIONS IMPORTANTES:
- Concentre-toi UNIQUEMENT sur les organisations en lien direct avec le TITRE de l'article
- Ignore les organisations mentionn√©es dans les publicit√©s, les suggestions d'articles ou les contenus non li√©s au titre
- Extrais TOUTES les organisations qui sont ACTEURS dans l'article (pas juste mentionn√©es passivement)
- Types recherch√©s:
  * Soci√©t√© civile: syndicats, associations, ordres professionnels, coalitions, OBNL, groupes de citoyens
  * Gouvernement: partis politiques, gouvernement du Qu√©bec, gouvernement f√©d√©ral, minist√®res, organismes publics
  * Municipalit√©s et administrations locales
  * Entreprises et secteur priv√© si elles prennent position publiquement
- IMPORTANT: Inclure les partis politiques au pouvoir (ex: CAQ, gouvernement Legault) quand ils proposent des mesures, projets de loi, politiques

Pour chaque organisation pertinente, extrais:
1. NOM: Nom complet de l'organisation (ex: "Coalition Avenir Qu√©bec (CAQ)", "Gouvernement du Qu√©bec", "F√©d√©ration des travailleurs du Qu√©bec")
2. TYPE: Type d'organisation (parti politique, gouvernement, minist√®re, syndicat, association, ordre professionnel, coalition, OBNL, municipalit√©, entreprise, etc.)
3. ACTION: Action principale men√©e (propose, d√©pose, d√©nonce, demande, pr√©sente un m√©moire, r√©agit, s'inqui√®te, critique, annonce, r√©forme, etc.)
4. ENJEU: Enjeu ou sujet principal en 5-10 mots
5. CITATION: Extrait textuel o√π l'organisation est mentionn√©e (1-2 phrases cl√©s du contenu original)
6. RESUME: Mini-r√©sum√© de l'implication de l'organisation en 15-25 mots

Si aucune organisation n'est acteur EN LIEN AVEC LE TITRE, r√©ponds avec une liste vide.

**T√ÇCHE 2: R√âSUM√â DE L'ARTICLE**
R√©dige un r√©sum√© concis de l'article en 3-4 phrases qui capture:
- Le sujet principal
- Les acteurs cl√©s
- L'enjeu ou la probl√©matique
- La position ou action principale

**IMPORTANT - FORMAT JSON:**
- R√©ponds UNIQUEMENT en JSON valide
- √âchappe les guillemets dans les textes avec \"
- Remplace les apostrophes par des espaces simples
- Pas de retours √† la ligne dans les valeurs de texte

R√©ponds UNIQUEMENT en format JSON:
{{
  "resume_article": "R√©sum√© de 3-4 phrases...",
  "organisations": [
    {{
      "nom": "...",
      "type": "...",
      "action": "...",
      "enjeu": "...",
      "citation": "...",
      "resume": "..."
    }}
  ]
}}
"""


def initialize_gemini():
    """Initialise l'API Gemini"""
    api_key = os.environ.get("GEMINI_API_KEY")

    if not api_key:
        print("‚ùå Erreur: Variable d'environnement GEMINI_API_KEY non d√©finie")
        print("   Ex√©cutez: export GEMINI_API_KEY='votre_cl√©_api'")
        exit(1)

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(GEMINI_MODEL)
    return model


def extract_organizations_and_summary(model, titre: str, contenu: str) -> dict:
    """Extrait les organisations ET le r√©sum√© d'un article avec Gemini"""
    try:
        # Limiter le contenu √† 3000 caract√®res pour √©viter tokens excessifs
        contenu_tronque = contenu[:3000] if len(contenu) > 3000 else contenu

        prompt = EXTRACTION_PROMPT.format(titre=titre, contenu=contenu_tronque)
        response = model.generate_content(prompt)

        # Parser la r√©ponse JSON
        response_text = response.text.strip()

        # Nettoyer le markdown si pr√©sent
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.startswith("```"):
            response_text = response_text[3:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]

        response_text = response_text.strip()

        # Tenter de parser le JSON
        try:
            data = json.loads(response_text)
        except json.JSONDecodeError as json_err:
            # Tentative de nettoyage du JSON
            print(f"   ‚ö†Ô∏è  JSON invalide, tentative de nettoyage...")

            # Nettoyer les retours √† la ligne dans les cha√Ænes
            import re
            # Remplacer les retours √† la ligne entre guillemets par des espaces
            response_text = re.sub(r'"\s*\n\s*', '" ', response_text)

            # Retry le parsing
            try:
                data = json.loads(response_text)
                print(f"   ‚úÖ JSON nettoy√© et pars√© avec succ√®s")
            except json.JSONDecodeError:
                # Si √ßa √©choue encore, logger et retourner vide
                print(f"   ‚ùå Impossible de parser le JSON: {str(json_err)[:100]}")
                # Sauvegarder le JSON probl√©matique pour debug
                debug_file = Path("debug_json_error.txt")
                with open(debug_file, 'a', encoding='utf-8') as f:
                    f.write(f"\n\n=== ERREUR POUR: {titre[:50]} ===\n")
                    f.write(response_text)
                    f.write(f"\nERREUR: {json_err}\n")
                return {"resume_article": "", "organisations": []}

        return {
            "resume_article": data.get("resume_article", ""),
            "organisations": data.get("organisations", [])
        }

    except Exception as e:
        print(f"   ‚ùå Erreur extraction: {str(e)[:100]}")
        return {"resume_article": "", "organisations": []}


def aggregate_organizations(all_extractions: list) -> dict:
    """Agr√®ge les organisations par nom"""
    org_dict = defaultdict(lambda: {
        "mentions": 0,
        "articles": [],
        "enjeux": set(),
        "signaux": set(),
        "types": set()
    })

    for extraction in all_extractions:
        for org_data in extraction["organisations"]:
            nom = org_data["nom"]

            org_dict[nom]["mentions"] += 1
            org_dict[nom]["articles"].append({
                "titre": extraction["article"]["titre"],
                "source": extraction["article"]["source"],
                "url": extraction["article"]["url"],
                "signal": extraction["article"]["signal"],
                "action": org_data["action"],
                "enjeu": org_data["enjeu"],
                "citation": org_data.get("citation", ""),
                "resume": org_data.get("resume", "")
            })
            org_dict[nom]["enjeux"].add(org_data["enjeu"])
            org_dict[nom]["signaux"].add(extraction["article"]["signal"])
            org_dict[nom]["types"].add(org_data["type"])

    # Convertir en format final
    organisations = []
    for nom, data in org_dict.items():
        organisations.append({
            "nom": nom,
            "type": list(data["types"])[0] if len(data["types"]) == 1 else ", ".join(data["types"]),
            "mentions": data["mentions"],
            "articles": data["articles"],
            "enjeux_principaux": list(data["enjeux"]),
            "signaux": list(data["signaux"])
        })

    # Trier par nombre de mentions
    organisations.sort(key=lambda x: x["mentions"], reverse=True)

    return {"organisations": organisations}


def process_article(article_data):
    """Traite un article (pour parall√©lisation)"""
    i, article, model = article_data

    print(f"[{i}] {article['titre'][:60]}...")

    result = extract_organizations_and_summary(model, article['titre'], article['contenu'])

    extraction_data = {
        "article": {
            "titre": article['titre'],
            "source": article['source'],
            "url": article['url'],
            "signal": article['signal'],
            "resume": result["resume_article"]
        },
        "organisations": result["organisations"]
    }

    if result["organisations"]:
        print(f"[{i}] ‚úÖ {len(result['organisations'])} organisation(s) extraite(s)")
    else:
        print(f"[{i}] ‚ö†Ô∏è  Aucune organisation identifi√©e")

    return extraction_data


def main():
    """Extrait et agr√®ge les organisations avec traitement parall√®le"""
    date_str = datetime.now().strftime("%Y-%m-%d")

    input_file = Path("data/warehouse") / f"google_news_{date_str}.csv"
    output_orgs_file = Path("data/warehouse") / f"google_news_organizations_{date_str}.json"
    output_summaries_file = Path("data/warehouse") / f"google_news_summaries_{date_str}.json"

    # Cr√©er dossier pour extractions par article
    extractions_dir = Path("data/warehouse/org_extraites") / date_str
    extractions_dir.mkdir(parents=True, exist_ok=True)

    if not input_file.exists():
        print(f"‚ùå Fichier introuvable: {input_file}")
        print(f"   Ex√©cutez d'abord: python processors/google_news/3_build_warehouse.py")
        return

    print(f"üîç Extraction des organisations et r√©sum√©s - {date_str}\n")

    # Initialiser Gemini
    model = initialize_gemini()
    print("‚úÖ API Gemini initialis√©e")
    print(f"‚ö° Traitement parall√®le avec {MAX_WORKERS} workers\n")

    # Lire les articles
    articles = []
    with open(input_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        articles = list(reader)

    print(f"üì∞ {len(articles)} articles √† analyser\n")

    # Pr√©parer les donn√©es pour traitement parall√®le
    article_tasks = [(i+1, article, model) for i, article in enumerate(articles)]

    # Traiter en parall√®le avec ThreadPoolExecutor
    all_extractions_data = []
    all_extractions = []
    summaries = []

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Soumettre toutes les t√¢ches
        future_to_article = {executor.submit(process_article, task): task for task in article_tasks}

        # R√©cup√©rer les r√©sultats au fur et √† mesure
        for future in as_completed(future_to_article):
            extraction_data = future.result()
            all_extractions_data.append(extraction_data)

            # Sauvegarder extraction par article
            article_num = len(all_extractions_data)
            article_file = extractions_dir / f"article_{article_num:04d}.json"
            with open(article_file, 'w', encoding='utf-8') as f:
                json.dump(extraction_data, f, ensure_ascii=False, indent=2)

            # Pr√©parer pour agr√©gation
            if extraction_data["organisations"]:
                all_extractions.append({
                    "article": extraction_data["article"],
                    "organisations": extraction_data["organisations"]
                })

            # Collecter r√©sum√©
            summaries.append({
                "titre": extraction_data["article"]["titre"],
                "source": extraction_data["article"]["source"],
                "url": extraction_data["article"]["url"],
                "signal": extraction_data["article"]["signal"],
                "resume": extraction_data["article"]["resume"]
            })

    print()

    # Agr√©ger par organisation
    print("üìä Agr√©gation des organisations...\n")
    aggregated_orgs = aggregate_organizations(all_extractions)

    # Sauvegarder le JSON des organisations
    with open(output_orgs_file, 'w', encoding='utf-8') as f:
        json.dump(aggregated_orgs, f, ensure_ascii=False, indent=2)

    # Sauvegarder le JSON des r√©sum√©s
    with open(output_summaries_file, 'w', encoding='utf-8') as f:
        json.dump({"articles": summaries}, f, ensure_ascii=False, indent=2)

    # R√©sum√©
    print(f"{'='*60}")
    print(f"‚úÖ Extraction termin√©e!")
    print(f"üìä R√©sultats:")
    print(f"   Organisations uniques: {len(aggregated_orgs['organisations'])}")
    print(f"   Articles avec organisations: {len(all_extractions)}/{len(articles)}")
    print(f"   Fichier organisations: {output_orgs_file}")
    print(f"   Fichier r√©sum√©s: {output_summaries_file}")
    print(f"   Extractions par article: {extractions_dir}/")

    # Top 10 organisations
    if aggregated_orgs['organisations']:
        print(f"\nüèÜ Top 10 organisations (par mentions):")
        for i, org in enumerate(aggregated_orgs['organisations'][:10], 1):
            print(f"   {i}. {org['nom']} ({org['mentions']} mentions)")

    print(f"\n‚úÖ Pr√™t pour analyse Stage 2 (Claude Code)")


if __name__ == "__main__":
    main()
