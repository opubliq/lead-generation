#!/usr/bin/env python3
"""
√âtape 2: Filtre les articles avec Gemini API selon leur pertinence
Input:  data/lake/google_news_rss/<date>/articles_raw.csv
Output: data/lake/google_news_filtered/<date>/articles_filtered.csv
"""

import csv
from pathlib import Path
from datetime import datetime
import os
import google.generativeai as genai
from time import sleep


# Configuration Gemini
GEMINI_MODEL = "gemini-2.5-flash"  # Rapide, stable, √©conomique
PERTINENCE_THRESHOLD = 4  # Garder seulement scores >= 4

# Prompt pour l'√©valuation
EVALUATION_PROMPT = """Tu travailles au d√©veloppement des affaires chez Opubliq et tu analyses les nouvelles pour identifier des clients potentiels.

Opubliq (https://opubliq.com/) est une firme sp√©cialis√©e dans la transformation de donn√©es d'opinion publique en actions strat√©giques. Services offerts:
- Analyse de donn√©es d'opinion publique et recherche sur mesure
- Strat√©gie de campagnes politiques et √©lectorales
- Mesure d'acceptabilit√© sociale et analyse de sentiment
- D√©veloppement de donateurs et strat√©gies de financement

Clients types: partis politiques, candidats, OBNL, groupes de pression, organisations avec enjeux d'acceptabilit√© sociale.

√âvalue la pertinence de cet article pour identifier des clients potentiels.

Titre: {titre}
Source: {source}

Crit√®res de pertinence:
- Score 5: Organisation menant campagne publique/politique, besoin √©vident de donn√©es d'opinion ou strat√©gie √©lectorale (ex: parti politique, groupe de pression en campagne, organisation avec enjeu majeur d'acceptabilit√© sociale)
- Score 4: Organisation positionn√©e publiquement sur enjeu politique/social n√©cessitant potentiellement recherche d'opinion (ex: association prenant position publique, OBNL sur enjeu controvers√©, groupe mobilisant des membres)
- Score 3: Organisation mentionn√©e dans contexte politique mais besoin indirect de recherche d'opinion
- Score 2: Mention organisationnelle marginale sans lien avec strat√©gie/opinion publique
- Score 1: Aucun lien avec les services d'Opubliq

IMPORTANT: R√©ponds UNIQUEMENT avec un chiffre entre 1 et 5. Aucun texte explicatif."""


def initialize_gemini():
    """Initialise l'API Gemini avec la cl√© d'environnement"""
    api_key = os.environ.get("GEMINI_API_KEY")

    if not api_key:
        print("‚ùå Erreur: Variable d'environnement GEMINI_API_KEY non d√©finie")
        print("   Ex√©cutez: export GEMINI_API_KEY='votre_cl√©_api'")
        exit(1)

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel(GEMINI_MODEL)
    return model


def evaluate_article(model, titre: str, source: str) -> int:
    """√âvalue la pertinence d'un article avec Gemini"""
    try:
        prompt = EVALUATION_PROMPT.format(titre=titre, source=source)
        response = model.generate_content(prompt)

        # Extraire le score (devrait √™tre juste un chiffre)
        score_text = response.text.strip()
        score = int(score_text)

        if score < 1 or score > 5:
            print(f"   ‚ö†Ô∏è  Score invalide ({score}), d√©faut √† 3")
            return 3

        return score

    except Exception as e:
        print(f"   ‚ùå Erreur √©valuation: {e}")
        return 3  # Score par d√©faut en cas d'erreur


def main():
    """Filtre les articles avec Gemini API"""
    date_str = datetime.now().strftime("%Y-%m-%d")
    input_file = Path("data/lake/google_news_rss") / date_str / "articles_raw.csv"

    if not input_file.exists():
        print(f"‚ùå Fichier introuvable: {input_file}")
        print(f"   Ex√©cutez d'abord: python processors/google_news/1_parse_rss.py")
        return

    # Cr√©er le dossier de sortie
    output_dir = Path("data/lake/google_news_filtered") / date_str
    output_dir.mkdir(parents=True, exist_ok=True)
    output_file = output_dir / "articles_filtered.csv"

    print(f"ü§ñ Filtrage des articles avec Gemini - {date_str}")
    print(f"üìä Seuil de pertinence: >= {PERTINENCE_THRESHOLD}\n")

    # Initialiser Gemini
    model = initialize_gemini()
    print("‚úÖ API Gemini initialis√©e\n")

    # Lire les articles
    articles = []
    with open(input_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        articles = list(reader)

    print(f"üì∞ {len(articles)} articles √† √©valuer\n")

    # √âvaluer chaque article
    filtered_articles = []
    scores_distribution = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}

    for i, article in enumerate(articles, 1):
        print(f"[{i}/{len(articles)}] {article['titre'][:60]}...")

        score = evaluate_article(model, article['titre'], article['source'])
        scores_distribution[score] += 1

        print(f"   Score: {score}/5")

        if score >= PERTINENCE_THRESHOLD:
            article['pertinence_llm'] = score
            filtered_articles.append(article)
            print(f"   ‚úÖ Conserv√©")
        else:
            print(f"   ‚ùå Rejet√©")

        # Rate limiting: pause courte entre appels
        sleep(0.5)
        print()

    # Sauvegarder les articles filtr√©s
    if filtered_articles:
        with open(output_file, 'w', newline='', encoding='utf-8') as f:
            fieldnames = ['signal', 'titre', 'source', 'url', 'pertinence_llm']
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(filtered_articles)

    # R√©sum√©
    print(f"{'='*60}")
    print(f"‚úÖ Filtrage termin√©!")
    print(f"üìä R√©sultats:")
    print(f"   Articles initiaux: {len(articles)}")
    print(f"   Articles conserv√©s: {len(filtered_articles)} ({len(filtered_articles)/len(articles)*100:.1f}%)")
    print(f"   Fichier: {output_file}")

    print(f"\nüìà Distribution des scores:")
    for score in range(5, 0, -1):
        count = scores_distribution[score]
        bar = "‚ñà" * (count // 2)
        print(f"   {score}: {bar} {count} articles")

    print(f"\n‚û°Ô∏è  Prochaine √©tape: python processors/google_news/3_download_html.py")


if __name__ == "__main__":
    main()
