#!/usr/bin/env python3
"""
√âtape 2: T√©l√©charge les HTMLs de tous les articles
Input:  data/lake/google_news_rss/<date>/articles_raw.csv
Output: data/lake/google_news_html/<date>/article_*.html
"""

import csv
from pathlib import Path
from datetime import datetime
import requests
from time import sleep
from urllib.parse import urlparse


# Domaines qu√©b√©cois/canadiens accept√©s
ALLOWED_DOMAINS = {
    # M√©dias qu√©b√©cois
    'lapresse.ca', 'ledevoir.com', 'journaldemontreal.com', 'journaldequebec.com',
    'tvanouvelles.ca', 'radio-canada.ca', 'ici.radio-canada.ca', 'rcinet.ca',
    'lactualite.com', 'ledroit.com', 'lesoleil.com', 'latribune.ca',
    'nouvelliste.ca', 'lequotidien.com', 'lavoixdelest.ca',
    # Gouvernement et institutions qu√©b√©coises
    'quebec.ca', 'gouv.qc.ca', 'assnat.qc.ca', 'dgeq.org',
    # Ordres professionnels et organisations qu√©b√©coises
    'oiiq.org', 'cmq.org', 'barreau.qc.ca', 'opq.gouv.qc.ca',
    # Syndicats et associations qu√©b√©coises
    'csn.qc.ca', 'ftq.qc.ca', 'fiq.qc.ca', 'scfp.qc.ca', 'sqees-298.qc.ca',
    # Municipalit√©s et r√©gions
    'ville.montreal.qc.ca', 'ville.quebec.qc.ca', 'stm.info',
    # M√©dias locaux et r√©gionaux
    'monvicto.com', 'lienmultimedia.com', 'francopresse.ca',
    # M√©dias canadiens g√©n√©ralistes
    'cbc.ca', 'theglobeandmail.com', 'nationalpost.com', 'thestar.com',
    'globalnews.ca', 'ctv.ca', 'ctvnews.ca'
}


def is_quebec_canadian_domain(url: str) -> bool:
    """V√©rifie si l'URL est d'un domaine qu√©b√©cois/canadien accept√©"""
    try:
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        # Retirer 'www.' si pr√©sent
        if domain.startswith('www.'):
            domain = domain[4:]

        # Accepter tous les .ca (canadiens par d√©faut)
        if domain.endswith('.ca'):
            return True

        # V√©rifier si le domaine exact est dans la liste
        if domain in ALLOWED_DOMAINS:
            return True

        # V√©rifier si c'est un sous-domaine d'un domaine accept√©
        for allowed in ALLOWED_DOMAINS:
            if domain.endswith('.' + allowed):
                return True

        return False
    except:
        return False


def download_html(url: str, output_file: Path, timeout: int = 30) -> bool:
    """T√©l√©charge le HTML d'une URL"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }

        response = requests.get(url, headers=headers, timeout=timeout, allow_redirects=True)
        response.raise_for_status()

        # Sauvegarder le HTML
        output_file.write_text(response.text, encoding='utf-8')
        return True

    except requests.exceptions.Timeout:
        print(f"   ‚è±Ô∏è  Timeout")
        return False
    except requests.exceptions.HTTPError as e:
        print(f"   ‚ùå Erreur HTTP: {e.response.status_code}")
        return False
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")
        return False


def sanitize_filename(url: str, article_id: int) -> str:
    """Cr√©e un nom de fichier s√ªr bas√© sur l'ID"""
    return f"article_{article_id:04d}.html"


def main():
    """T√©l√©charge les HTMLs de tous les articles"""
    date_str = datetime.now().strftime("%Y-%m-%d")
    input_file = Path("data/lake/google_news_rss") / date_str / "articles_raw.csv"

    if not input_file.exists():
        print(f"‚ùå Fichier introuvable: {input_file}")
        print(f"   Ex√©cutez d'abord: python processors/google_news/1_parse_rss.py")
        return

    # Cr√©er le dossier de sortie
    output_dir = Path("data/lake/google_news_html") / date_str
    output_dir.mkdir(parents=True, exist_ok=True)

    print(f"üåê T√©l√©chargement des HTMLs - {date_str}")
    print(f"üìÅ Destination: {output_dir}\n")

    # Lire tous les articles
    articles = []
    with open(input_file, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        articles = list(reader)

    print(f"üì∞ {len(articles)} articles √† filtrer et t√©l√©charger\n")

    # Filtrer et t√©l√©charger chaque HTML
    success_count = 0
    skipped_count = 0
    failed_urls = []
    filtered_articles = []

    for i, article in enumerate(articles, 1):
        url = article['url']

        print(f"[{i}/{len(articles)}] {article['titre'][:50]}...")
        print(f"   URL: {url[:70]}...")

        # Filtrer par domaine
        if not is_quebec_canadian_domain(url):
            print(f"   ‚è≠Ô∏è  Ignor√© (domaine √©tranger)")
            skipped_count += 1
            print()
            continue

        filename = sanitize_filename(url, len(filtered_articles) + 1)
        output_file = output_dir / filename

        # T√©l√©charger
        success = download_html(url, output_file)

        if success:
            file_size = output_file.stat().st_size
            print(f"   ‚úÖ T√©l√©charg√©: {filename} ({file_size:,} bytes)")
            success_count += 1

            # Ajouter le nom de fichier √† l'article pour r√©f√©rence
            article['html_file'] = filename
            filtered_articles.append(article)
        else:
            failed_urls.append({
                'titre': article['titre'],
                'url': url
            })

        # Rate limiting
        sleep(1)
        print()

    # Mettre √† jour la liste des articles avec seulement ceux t√©l√©charg√©s
    articles = filtered_articles

    # Cr√©er un mapping CSV pour r√©f√©rence
    mapping_file = output_dir / "articles_mapping.csv"
    with open(mapping_file, 'w', newline='', encoding='utf-8') as f:
        fieldnames = ['signal', 'titre', 'source', 'url', 'html_file']
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(articles)

    # R√©sum√©
    print(f"{'='*60}")
    print(f"‚úÖ T√©l√©chargement termin√©!")
    print(f"üìä T√©l√©charg√©s: {success_count} | Ignor√©s: {skipped_count} | √âchecs: {len(failed_urls)}")
    print(f"‚û°Ô∏è  Prochaine √©tape: python processors/google_news/3_build_warehouse.py")


if __name__ == "__main__":
    main()
