#!/usr/bin/env python3
"""
Google News RSS Scraper for Lead Generation
Collecte les articles des 7 derniers jours selon 2 signaux optimis√©s
"""

import requests
from datetime import datetime
from pathlib import Path
import urllib.parse


# Configuration des 2 recherches par signal (√©largies pour minimiser faux n√©gatifs)
SEARCH_QUERIES = {
    "organisations_action_legislative": {
        "query": "(association OR f√©d√©ration OR coalition OR ordre OR syndicat OR regroupement OR conseil OR collectif) (t√©moigne OR m√©moire OR demande OR r√©clame OR appelle OR d√©nonce OR r√©agit OR s'oppose OR critique OR conteste OR interpelle OR exige) (Qu√©bec OR gouvernement qu√©b√©cois OR ministre) when:7d",
        "description": "Organisations en action l√©gislative - requ√™te large"
    },
    "engagement_legislatif_organisationnel": {
        "query": "(projet de loi OR r√®glement OR consultation publique OR commission parlementaire) (association OR f√©d√©ration OR coalition OR ordre OR syndicat OR regroupement) (pr√©sente OR d√©pose OR recommande OR propose OR appuie OR critique OR s'inqui√®te OR d√©nonce) Qu√©bec when:7d",
        "description": "Engagement l√©gislatif organisationnel - requ√™te large"
    }
}

# Configuration Google News RSS
BASE_URL = "https://news.google.com/rss/search"
PARAMS = {
    "hl": "fr-CA",
    "gl": "CA",
    "ceid": "CA:fr"
}


def construct_rss_url(query: str) -> str:
    """Construit l'URL compl√®te pour Google News RSS"""
    params = PARAMS.copy()
    params["q"] = query
    return f"{BASE_URL}?{urllib.parse.urlencode(params)}"


def fetch_rss_feed(url: str) -> str:
    """R√©cup√®re le contenu XML d'un flux RSS"""
    headers = {
        "User-Agent": "Mozilla/5.0 (compatible; OpubliqLeadBot/1.0)"
    }

    response = requests.get(url, headers=headers, timeout=30)
    response.raise_for_status()
    return response.text


def save_rss_content(content: str, signal_name: str, date_str: str) -> Path:
    """Sauvegarde le contenu RSS dans le data lake"""
    output_dir = Path("data/lake/google_news_rss") / date_str
    output_dir.mkdir(parents=True, exist_ok=True)

    output_file = output_dir / f"{signal_name}.xml"
    output_file.write_text(content, encoding="utf-8")

    return output_file


def main():
    """Collecte tous les flux RSS et les sauvegarde"""
    date_str = datetime.now().strftime("%Y-%m-%d")

    print(f"üîç Collecte des flux Google News RSS - {date_str}")
    print(f"üìÅ Destination: data/lake/google_news_rss/{date_str}/\n")

    results = []

    for signal_name, config in SEARCH_QUERIES.items():
        print(f"üì∞ Signal: {signal_name}")
        print(f"   Description: {config['description']}")
        print(f"   Query: {config['query'][:80]}...")

        try:
            # Construire l'URL et r√©cup√©rer le flux
            url = construct_rss_url(config["query"])
            print(f"   URL: {url[:100]}...")

            content = fetch_rss_feed(url)

            # Sauvegarder
            output_file = save_rss_content(content, signal_name, date_str)
            file_size = len(content)

            print(f"   ‚úÖ Sauvegard√©: {output_file} ({file_size:,} bytes)")
            results.append({
                "signal": signal_name,
                "file": output_file,
                "size": file_size,
                "success": True
            })

        except Exception as e:
            print(f"   ‚ùå Erreur: {e}")
            results.append({
                "signal": signal_name,
                "success": False,
                "error": str(e)
            })

        print()

    # R√©sum√©
    successful = sum(1 for r in results if r.get("success"))
    total = len(results)
    total_size = sum(r.get("size", 0) for r in results if r.get("success"))

    print(f"\nüìä R√©sum√©:")
    print(f"   R√©ussis: {successful}/{total}")
    print(f"   Taille totale: {total_size:,} bytes")
    print(f"   Dossier: data/lake/google_news_rss/{date_str}/")


if __name__ == "__main__":
    main()
